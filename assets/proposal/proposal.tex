\documentclass{article}

% Packages for equations, fonts, and colors.
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{xcolor}

% Packages and Commands for list head.
\usepackage{enumitem}
\renewcommand*{\thesection}{\Roman{section}}
\renewcommand*{\thesubsection}{\thesection.(\alph{subsection})}

% Packages and Commands for size.
\usepackage{geometry}
\geometry{
a4paper,
left=20mm,
right=20mm,
top=15mm,
bottom=20mm
}
% Packages and Commands to set up paragraph indentation.
\usepackage{lipsum}
\usepackage{setspace}
\setlength{\parindent}{0em}
\setlength{\parskip}{1.0em}

% Packages and Commands to set up citation, links, etc.
\usepackage{hyperref}
\hypersetup{
colorlinks=true,
citecolor=blue,
linkcolor=blue,
filecolor=magenta,
urlcolor=blue
}

% Packages and commands for title spacing.
\usepackage{titlesec}
\titlespacing*{\section}
{0cm}{0.1cm}{0.1cm}
\titlespacing*{\subsection}
{0.25cm}{0.1cm}{0.1cm}

% Commands to set up reference template.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% ------------------------------------------------------------

\title{Project Proposal: Digits Classification}
\author{Team 5\footnote{Contact: \{shiwechen6-c, sxwang6-c\}@my.cityu.edu.hk}  \:- Shiwei Chen, Shixiang Wang}
\date{March 2022}

\begin{document}

\maketitle

\section{Introduction}
 In this project, we will research on classifying handwritten digits, a subtask under the field of Contextual Image Classification in Computer Vision. The task is provided by the famous Modified National Institute of Standard and Technology datasets \cite{ref1} (Hereinafter referred as \textit{the MNIST dataset}). In the early days of machine learning, the dataset was widely used for benchmarking various image processing systems. While the original datasets contain $60000$ samples for training and $10000$ samples for testing, our project uses only a fraction of them, which are $2000$ samples for training and $2000$ samples for testing. The fewer data makes our project even more challenging. Additionally, the project will be tested on a hidden dataset at the end of the submission deadline to determine the final performance. In the subsequent sections, we will describe our method for the task, the basic structure of the experiment, and the expected outcome.

\section{Methodology}
 Due to the restricted size of the data, we deem that Data Augmentation, a technique to increase the diversity of the training data without actual collection and labeling, is necessary. A large amount of data is essential for supervised learning methods to achieve high-performance \cite{ref2}. We plan on using random noise, random rotation and translation, flipping (for symmetric digits), cropping, and resizing to at least increase the size of the restricted dataset to the same magnitude as the original MNIST dataset. After that, we will walk through the entire machine learning progress. First, we will try to use dimension reduction algorithms such as PCA to preprocess the feature vectors. Then we plan to try traditional classification algorithms such as logistic regression, SVM, and KNN. As this is a multi-class classification problem, we will try to use '1 vs. all' or 'softmax' to extend the binary classification to multi-class classification. According to the leaderboard, \cite{ref3}, Convolution Neural Network (CNN)\cite{ref4} is still the dominant component in the state of the arts of this task. Therefore, we plan on constructing our network structure based on CNN. More precisely, we want to construct an ensemble of slightly different CNN-based networks that will together vote for the final classification decision \cite{ref5}. The difference can be kernel sizes, number of layers, number of nodes, with or without normalization, etc. Besides, we will also try to apply residual blocks to alleviate the vanishing gradient problem in deep neural networks. We will test our implement algorithms on a separate test data set to ensure that the training set and test set are independent. The accuracy will be chosen as the evaluation criteria.

\section{Experiment and Expected Result}
The experiment will be conducted in a Jupyter notebook for convenience, with PyTorch being the primary network building tool. The experiment will follow general data science workflow and thus roughly consist of four parts: 
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \item Perform Exploratory Data Analysis (EDA) to get a general view of the data and see if there are some noteworthy characteristics.
    \item Conduct Data Augmentation base on the result of EDA.
    \item Construct the designed structure and implement the proposed method.
    \item Compare and analyze the final results and select the best one achieved.
\end{itemize}
The expected outcome would be our best model saved in a .pt or .pth file accompanied by a python script to reproduce the results, a notebook for visualizations, a final report, and a presentation video.

\begin{thebibliography}{00}
\bibitem{ref1}LeCun, Y., Bottou L., Bengio, Y., and Haffner, P. ``Gradient-based learning applied to document recognition." Proceedings of the IEEE, 86(11):2278-2324, November 1998.
\bibitem{ref2}E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, ``AutoAugment: Learning augmentation policies
from data", in Proc. of the Conference on Computer Vision and Pattern Recognition (CVPR), 113-123 (2019)
\bibitem{ref3} Meta AI, 2022. Access Date: March 21st, 2022. Url: https://paperswithcode.com/sota/image-classification-on-mnist.
\bibitem{ref4}LeCun, Y., Haffner, P., Bottou, L., Bengio, Y. (1999). ``Object Recognition with Gradient-Based Learning". Shape, Contour and Grouping in Computer Vision.
\bibitem{ref5}An, S., Lee, M., Park, S., Yang, H., So, J. (2020). ``An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit Recognition". 
\end{thebibliography}


\end{document}
