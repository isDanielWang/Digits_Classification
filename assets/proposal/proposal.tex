\documentclass{article}

% Packages for equations, fonts, and colors.
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{xcolor}

% Packages and Commands for list head.
\usepackage{enumitem}
\renewcommand*{\thesection}{\Roman{section}}
\renewcommand*{\thesubsection}{\thesection.(\alph{subsection})}

% Packages and Commands for size.
\usepackage{geometry}
\geometry{
a4paper,
left=20mm,
right=20mm,
top=20mm,
bottom=20mm
}
% Packages and Commands to set up paragraph indentation.
\usepackage{lipsum}
\usepackage{setspace}
\setlength{\parindent}{0em}
\setlength{\parskip}{1.2em}

% Packages and Commands to set up citation, links, etc.
\usepackage{hyperref}
\hypersetup{
colorlinks=true,
citecolor=blue,
linkcolor=blue,
filecolor=magenta,
urlcolor=blue
}

% Packages and commands for title spacing.
\usepackage{titlesec}
\titlespacing*{\section}
{0cm}{0.1cm}{0.1cm}
\titlespacing*{\subsection}
{0.25cm}{0.1cm}{0.1cm}

% Commands to set up reference template.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% ------------------------------------------------------------

\title{Project Proposal: Digits Classification using CNN}
\author{Team 5\footnote{Contact: \{shiwechen6-c, sxwang6-c\}@my.cityu.edu.hk}  \:- Shiwei Chen, Shixiang Wang}
\date{March 2022}

\begin{document}

\maketitle

\section{Introduction}
In this project, we will research on classifying handwritten digits, a subtask under the field of Contextual Image Classification in Computer Vision. The task is provided by the famous Modified National Institute of Standard and Technology datasets \cite{ref1} (Hereinafter referred as \textit{the MNIST dataset}). In the early days of machine learning, the dataset was widely used for benmarking various image processing systems. While the original datasets contains $60000$ samples for training and $10000$ samples for testing, our project uses only a fraction of them, which are $2000$ samples for training and $2000$ samples for testing. Additionally, the project will be tested on a hidden dataset at the end of submission deadline to determine the final performance. In the subsequent sections, we will describe our method for the task, the basic structure of the experiment, and the expected outcome.

\section{Methodology}
    \subsection{Data Augmentation}
    Due to the restricted size of the data, we deem that Data Augmentation, a techinique to increase the diversity of the training data without actual collection and labeling, is necessary. Large amount of data is essential for supervised learning methods to achieve high performance \cite{ref2}. We plan on using random noise, random rotation and translation, flipping (for symmetric digits), cropping, and resizing, to at least increase the size of the restricted dataset to the same magnitude as the original MNIST dataset.
    \subsection{Network Structure}
    According to the leaderboard \cite{ref3}, Convolution Neural Network (CNN)\cite{ref4} is still the dominant component in the state of the arts of this task. Therefore, we plan on construct our own netowrk structure on the basis of CNN. More precisely, we want to construct an ensemble of slightly different CNN-based networks that will together vote for the final classification decision \cite{ref5}. The difference can be kernel sizes, number of layers, with or without normalization, etc. 

\section{Experiment and Expected Result}
The experiment will be conducted in a Jupyter notebook for convenience, with PyTorch being the primary network building tool. The experiment will follow general data science workflow and thus roughly consist of four parts: 
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \item Perform Exploratory Data Analysis (EDA) to get a general view of the data and see if there are some noteworthy characteristics.
    \item Conduct Data Augmentation base on the result of EDA.
    \item Construct the designed structure and implement the proposed method.
    \item Compare and analyze the final results and select the best one achieved.
\end{itemize}
The expected outcome would be our best model saved in a .pt or .pth file accompanied by a python script to reproduce the results, a notebook for visualizations, a final report, and a presentation video.

\begin{thebibliography}{00}
\bibitem{ref1}LeCun, Y., Bottou L., Bengio, Y., and Haffner, P. ``Gradient-based learning applied to document recognition." Proceedings of the IEEE, 86(11):2278-2324, November 1998.
\bibitem{ref2}E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, ``AutoAugment: Learning augmentation policies
from data", in Proc. of the Conference on Computer Vision and Pattern Recognition (CVPR), 113-123 (2019)
\bibitem{ref3} Meta AI, 2022. Access Date: March 21st, 2022. Url: https://paperswithcode.com/sota/image-classification-on-mnist.
\bibitem{ref4}LeCun, Y., Haffner, P., Bottou, L., Bengio, Y. (1999). ``Object Recognition with Gradient-Based Learning". Shape, Contour and Grouping in Computer Vision.
\bibitem{ref5}An, S., Lee, M., Park, S., Yang, H., So, J. (2020). ``An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit Recognition". 
\end{thebibliography}


\end{document}